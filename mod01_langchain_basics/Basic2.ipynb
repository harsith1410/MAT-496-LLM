{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527f8e31",
   "metadata": {},
   "source": [
    "Source: https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3515867",
   "metadata": {},
   "source": [
    "- get your llm api key\n",
    "- Use Groq - its free\n",
    "- save it in .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01dc67e",
   "metadata": {},
   "source": [
    "Also do these steps in .env\n",
    "\n",
    "LANGSMITH_TRACING=\"true\"\n",
    "LANGSMITH_API_KEY=\"...\"\n",
    "LANGSMITH_PROJECT=\"default\" # or any other project name"
   ]
  },
  {
   "cell_type": "code",
   "id": "682c4c87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:29.202869Z",
     "start_time": "2025-08-26T09:10:29.186352Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "afe84af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T10:00:29.205426Z",
     "start_time": "2025-08-21T10:00:19.879001Z"
    }
   },
   "source": "# pip install -U langchain-groq",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Obtaining dependency information for langchain-groq from https://files.pythonhosted.org/packages/9d/31/5f32d15105d0160c3753113d5cca5787236c6d2717d25126597d8adc39dd/langchain_groq-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-groq) (0.3.74)\n",
      "Collecting groq<1,>=0.30.0 (from langchain-groq)\n",
      "  Obtaining dependency information for groq<1,>=0.30.0 from https://files.pythonhosted.org/packages/ab/f8/14672d69a91495f43462c5490067eeafc30346e81bda1a62848e897f9bc3/groq-0.31.0-py3-none-any.whl.metadata\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq<1,>=0.30.0->langchain-groq)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (4.14.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (25.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain-groq) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsi\\pycharmprojects\\pythonlearning\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n",
      "Downloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n",
      "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.4 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 71.7/131.4 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 131.4/131.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq, langchain-groq\n",
      "Successfully installed distro-1.9.0 groq-0.31.0 langchain-groq-0.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b38fb03b76ff8f37"
  },
  {
   "cell_type": "code",
   "id": "a0e461e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:33.398384Z",
     "start_time": "2025-08-26T09:10:32.899170Z"
    }
   },
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a6f93f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:50.701914Z",
     "start_time": "2025-08-26T09:10:50.361759Z"
    }
   },
   "source": [
    "# A simple model call\n",
    "\n",
    "response = model.invoke(\"Who are you ?\")\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text based on the input I receive.\n",
      "\n",
      "I was trained on a massive dataset of text from various sources, which enables me to understand and respond to a wide range of topics and questions. My primary goal is to provide helpful and accurate information to those who interact with me, while also being friendly and engaging.\n",
      "\n",
      "I'm constantly learning and improving my responses based on the conversations I have with users like you. So, feel free to ask me anything, and I'll do my best to provide a helpful and informative response!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "42416cbd",
   "metadata": {},
   "source": [
    "[Exercise] Play along. Give bigger and bigger prompts"
   ]
  },
  {
   "cell_type": "code",
   "id": "da2738af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:44.491662Z",
     "start_time": "2025-08-26T09:10:44.324334Z"
    }
   },
   "source": [
    "model.invoke(\"Who are you?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm LLaMA, I'm a large language model trained by a team of researcher at Meta AI.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 14, 'total_tokens': 37, 'completion_time': 0.021428354, 'prompt_time': 0.002508518, 'queue_time': 0.045431492, 'total_time': 0.023936872}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--48525188-9881-4598-80eb-4ec719d19f20-0', usage_metadata={'input_tokens': 14, 'output_tokens': 23, 'total_tokens': 37})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a7f4578d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:55.410427Z",
     "start_time": "2025-08-26T09:10:54.943215Z"
    }
   },
   "source": [
    "response = model.invoke(\"\"\" Write a python code which can multiply two matrix of arbitrary but compatible order. The code should throw exception if the matrix dimentsions are not compatible for multiplication             \n",
    "             \"\"\")\n",
    "\n",
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a Python code that multiplies two matrices of arbitrary but compatible order:\\n\\n```\\nclass Matrix:\\n    def __init__(self, matrix):\\n        self.matrix = matrix\\n\\n    def multiply(self, other):\\n        if len(self.matrix[0]) != len(other.matrix):\\n            raise Exception(\"Matrices are not compatible for multiplication\")\\n\\n        result = [[0 for _ in range(len(other.matrix[0]))] for _ in range(len(self.matrix))]\\n        for i in range(len(self.matrix)):\\n            for j in range(len(other.matrix[0])):\\n                for k in range(len(other.matrix)):\\n                    result[i][j] += self.matrix[i][k] * other.matrix[k][j]\\n        return result\\n\\n    def __str__(self):\\n        return str(self.matrix)\\n\\n\\n# Example usage:\\nmatrix1 = Matrix([[1, 2, 3], [4, 5, 6]])\\nmatrix2 = Matrix([[7, 8], [9, 10], [11, 12]])\\n\\nresult = Matrix(matrix1.multiply(matrix2))\\nprint(result)\\n```\\n\\nIn this code, we define a `Matrix` class that has a `multiply` method to multiply two matrices. The `multiply` method checks if the matrices are compatible for multiplication by checking if the number of columns in the first matrix is equal to the number of rows in the second matrix. If they are not compatible, it raises an exception.\\n\\nThe `multiply` method then creates a new matrix to store the result, and iterates over the elements of the matrices to perform the multiplication.\\n\\nThe `__str__` method is used to convert the matrix to a string for printing.\\n\\nIn the example usage, we create two matrices `matrix1` and `matrix2`, multiply them using the `multiply` method, and print the result.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 371, 'prompt_tokens': 43, 'total_tokens': 414, 'completion_time': 0.343056601, 'prompt_time': 0.020355913, 'queue_time': 0.046244177, 'total_time': 0.363412514}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f255c871-a245-45dd-ae6a-379a00abf8d9-0', usage_metadata={'input_tokens': 43, 'output_tokens': 371, 'total_tokens': 414})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "17c8c30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:10:59.163568Z",
     "start_time": "2025-08-26T09:10:59.159284Z"
    }
   },
   "source": [
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python code that multiplies two matrices of arbitrary but compatible order:\n",
      "\n",
      "```\n",
      "class Matrix:\n",
      "    def __init__(self, matrix):\n",
      "        self.matrix = matrix\n",
      "\n",
      "    def multiply(self, other):\n",
      "        if len(self.matrix[0]) != len(other.matrix):\n",
      "            raise Exception(\"Matrices are not compatible for multiplication\")\n",
      "\n",
      "        result = [[0 for _ in range(len(other.matrix[0]))] for _ in range(len(self.matrix))]\n",
      "        for i in range(len(self.matrix)):\n",
      "            for j in range(len(other.matrix[0])):\n",
      "                for k in range(len(other.matrix)):\n",
      "                    result[i][j] += self.matrix[i][k] * other.matrix[k][j]\n",
      "        return result\n",
      "\n",
      "    def __str__(self):\n",
      "        return str(self.matrix)\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "matrix1 = Matrix([[1, 2, 3], [4, 5, 6]])\n",
      "matrix2 = Matrix([[7, 8], [9, 10], [11, 12]])\n",
      "\n",
      "result = Matrix(matrix1.multiply(matrix2))\n",
      "print(result)\n",
      "```\n",
      "\n",
      "In this code, we define a `Matrix` class that has a `multiply` method to multiply two matrices. The `multiply` method checks if the matrices are compatible for multiplication by checking if the number of columns in the first matrix is equal to the number of rows in the second matrix. If they are not compatible, it raises an exception.\n",
      "\n",
      "The `multiply` method then creates a new matrix to store the result, and iterates over the elements of the matrices to perform the multiplication.\n",
      "\n",
      "The `__str__` method is used to convert the matrix to a string for printing.\n",
      "\n",
      "In the example usage, we create two matrices `matrix1` and `matrix2`, multiply them using the `multiply` method, and print the result.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8b215da3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:11:27.793648Z",
     "start_time": "2025-08-26T09:11:27.637519Z"
    }
   },
   "source": [
    "# invoking to build a converstation style call\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Bengali\"), # try punjabi, or any other Indian language\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='নমস্কার! (Namoskar!)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41, 'completion_time': 0.014675881, 'prompt_time': 0.005965619, 'queue_time': 0.04575597, 'total_time': 0.0206415}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bbbee4df-072f-4731-b755-4d93356da426-0', usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "9efe5fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:11:31.397713Z",
     "start_time": "2025-08-26T09:11:30.441687Z"
    }
   },
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Generate python code for given tasks\"),\n",
    "    HumanMessage(\"Find max of given n numbers\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "response"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a Python code that finds the maximum of a given n numbers:\\n```\\ndef find_max(n, numbers):\\n    return max(numbers)\\n\\n# Example usage:\\nn = 5\\nnumbers = [12, 34, 56, 78, 90]\\nmax_number = find_max(n, numbers)\\nprint(\"Maximum number is:\", max_number)\\n```\\nIn this code, the `find_max` function takes two arguments: `n` (the number of elements in the list) and `numbers` (a list of n numbers). The `max` function is used to find the maximum value in the list, and the result is returned by the `find_max` function.\\n\\nYou can also use the built-in `max` function in Python, which can take an iterable (such as a list) as an argument:\\n```\\nnumbers = [12, 34, 56, 78, 90]\\nmax_number = max(numbers)\\nprint(\"Maximum number is:\", max_number)\\n```\\nThis will also find the maximum value in the list and print it.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 221, 'prompt_tokens': 27, 'total_tokens': 248, 'completion_time': 0.203585936, 'prompt_time': 0.007424034, 'queue_time': 0.678643916, 'total_time': 0.21100997}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e4066149-208c-40eb-8954-159de9b530f4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 221, 'total_tokens': 248})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d36098aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:11:39.994150Z",
     "start_time": "2025-08-26T09:11:39.988147Z"
    }
   },
   "source": [
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python code that finds the maximum of a given n numbers:\n",
      "```\n",
      "def find_max(n, numbers):\n",
      "    return max(numbers)\n",
      "\n",
      "# Example usage:\n",
      "n = 5\n",
      "numbers = [12, 34, 56, 78, 90]\n",
      "max_number = find_max(n, numbers)\n",
      "print(\"Maximum number is:\", max_number)\n",
      "```\n",
      "In this code, the `find_max` function takes two arguments: `n` (the number of elements in the list) and `numbers` (a list of n numbers). The `max` function is used to find the maximum value in the list, and the result is returned by the `find_max` function.\n",
      "\n",
      "You can also use the built-in `max` function in Python, which can take an iterable (such as a list) as an argument:\n",
      "```\n",
      "numbers = [12, 34, 56, 78, 90]\n",
      "max_number = max(numbers)\n",
      "print(\"Maximum number is:\", max_number)\n",
      "```\n",
      "This will also find the maximum value in the list and print it.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "39caa649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T09:11:44.943510Z",
     "start_time": "2025-08-26T09:11:42.047251Z"
    }
   },
   "source": [
    "# streaming example\n",
    "import time\n",
    "\n",
    "for token in model.stream(\"hi\"):\n",
    "    time.sleep(0.1)\n",
    "    print(token.content, end=\"|\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi|!| It|'s| nice| to| meet| you|.| Is| there| something| I| can| help| you| with| or| would| you| like| to| chat|?||"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class discussion point: What is an LLM as a program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496-monsoon2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
